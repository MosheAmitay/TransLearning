{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.6.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (7.16.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.5.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.26.0)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (21.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (0.18.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (5.0.9)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (3.0.19)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (4.3.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.4->tensorboard->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from werkzeug>=0.11.15->tensorboard->keras-tuner) (0.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pillow\n",
    "!pip install scipy\n",
    "#from PIL import Image\n",
    "!pip install keras-tuner\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-df7f2cabfbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apt install libgl1-mesa-glx -y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "!apt install libgl1-mesa-glx -y\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import *\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "# dimensions of our images.\n",
    "\n",
    "#import imgaug.augmenters as img_aug\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Flatten, Dense, BatchNormalization, Dropout, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import DenseNet121,ResNet152V2,InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import apply_affine_transform\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def get_train_image_quant(directory):\n",
    "    datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator()\n",
    "    data = datagen.flow_from_directory(directory)\n",
    "    unique = np.unique(data.classes, return_counts=True)\n",
    "    labels_dict = dict(zip(unique[0], unique[1]))\n",
    "    print(labels_dict)\n",
    "    image_quant_per_class = []\n",
    "    for key in labels_dict:\n",
    "        for i in range(labels_dict[key]):\n",
    "            image_quant_per_class.append(key)\n",
    "    return image_quant_per_class\n",
    "\n",
    "def get_class_weights(labels, one_hot=False):\n",
    "        if one_hot is False:\n",
    "            n_classes = max(labels) + 1\n",
    "        else:\n",
    "            n_classes = len(labels[0])\n",
    "        class_counts = [0 for _ in range(int(n_classes))]\n",
    "        if one_hot is False:\n",
    "            for label in labels:\n",
    "                class_counts[label] += 1\n",
    "        else:\n",
    "            for label in labels:\n",
    "                class_counts[label.index(1)] += 1\n",
    "        return {i : (1. / class_counts[i]) * float(len(labels)) / float(n_classes) for i in range(int(n_classes))}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_noise(img):\n",
    "    #print(img.shape)\n",
    "    #seq = img_aug.Sequential([\n",
    "    #img_aug.Emboss(alpha=(0.2, 0.2), strength=(1.5, 1.5))])\n",
    "  #  cv2_imshow(img)\n",
    "    angle = random.choice([0, 90, 180, 270])\n",
    "    if angle != 0:\n",
    "        img = apply_affine_transform(img, theta=angle, fill_mode='nearest')\n",
    "    return img\n",
    "    \n",
    "\n",
    "\n",
    "def first_train(train_dir,val_dir,path_to_model):\n",
    "    img_width, img_height = 400, 400\n",
    "\n",
    "    train_data_dir = train_dir\n",
    "    validation_data_dir = val_dir\n",
    "\n",
    "    train_images_labels = get_train_image_quant(train_data_dir) # for weights balancing\n",
    "    print (get_class_weights(train_images_labels))\n",
    "\n",
    "\n",
    "    nb_train_samples = len(os.listdir(train_dir+\"/clean/\")) + len(os.listdir(train_dir+\"/with_carotid/\"))\n",
    "    nb_validation_samples = len(os.listdir(val_dir+\"/clean/\")) + len(os.listdir(val_dir+\"/with_carotid/\"))\n",
    "\n",
    "    epochs = 50\n",
    "    batch_size = 16\n",
    "\n",
    "    #path_to_model = \"gdrive/MyDrive/data_sets/panoramic/panoramic_firsttrain.h5\"\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        # horizontal_flip=True,\n",
    "        # vertical_flip=True,\n",
    "         rescale=1. / 255,\n",
    "      #   preprocessing_function=add_noise,\n",
    "         width_shift_range=[0,40], \n",
    "         zoom_range=[0.9,1.1],\n",
    "          rotation_range=15,\n",
    "          shear_range=0.1,\n",
    "           fill_mode='constant', cval=0\n",
    "           \n",
    "    )\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # top layer\n",
    "    base_model = InceptionResNetV2(\n",
    "            weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "            input_shape=input_shape,\n",
    "            include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # top layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    #auc = tf.keras.metrics.AUC(name='auc')\n",
    "    #auc = tfa.metrics.F1Score(num_classes=2,name='f1')\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy',f1_metric])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "    early = EarlyStopping(monitor='val_f1_metric', min_delta=0.01, \\\n",
    "        patience=10, verbose=1, mode='max')\n",
    "\n",
    "    checkpoint = ModelCheckpoint(path_to_model, monitor='val_f1_metric',verbose=1,   # val_recall_metric\n",
    "                                    save_best_only=True, save_weights_only=False, mode='max', save_freq='epoch')\n",
    "\n",
    "    #log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,  histogram_freq=1, write_graph=True, write_images=True) \n",
    "\n",
    " # 'val_accuracy'\n",
    "    model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=300,#epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[ early,checkpoint],\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples//batch_size,\n",
    "    class_weight=get_class_weights(train_images_labels)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning - Second train\n",
    "import datetime\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "def get_train_image_quant(directory):\n",
    "    datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator()\n",
    "    data = datagen.flow_from_directory(directory)\n",
    "    unique = np.unique(data.classes, return_counts=True)\n",
    "    labels_dict = dict(zip(unique[0], unique[1]))\n",
    "    print(labels_dict)\n",
    "    image_quant_per_class = []\n",
    "    for key in labels_dict:\n",
    "        for i in range(labels_dict[key]):\n",
    "            image_quant_per_class.append(key)\n",
    "    return image_quant_per_class\n",
    "\n",
    "def get_class_weights(labels, one_hot=False):\n",
    "        if one_hot is False:\n",
    "            n_classes = max(labels) + 1\n",
    "        else:\n",
    "            n_classes = len(labels[0])\n",
    "        class_counts = [0 for _ in range(int(n_classes))]\n",
    "        if one_hot is False:\n",
    "            for label in labels:\n",
    "                class_counts[label] += 1\n",
    "        else:\n",
    "            for label in labels:\n",
    "                class_counts[label.index(1)] += 1\n",
    "        return {i : (1. / class_counts[i]) * float(len(labels)) / float(n_classes) for i in range(int(n_classes))}\n",
    "\n",
    "def fine_tuning_train(train_dir,val_dir,previous_model_path, current_model_path):\n",
    "    img_width, img_height = 400, 400\n",
    "\n",
    "    train_data_dir = train_dir\n",
    "    validation_data_dir = val_dir\n",
    "\n",
    "    train_images_labels = get_train_image_quant(train_data_dir) # for weights balancing\n",
    "    print (get_class_weights(train_images_labels))\n",
    "\n",
    "\n",
    "    nb_train_samples = len(os.listdir(train_dir+\"/clean/\")) + len(os.listdir(train_dir+\"/with_carotid/\"))\n",
    "    nb_validation_samples = len(os.listdir(val_dir+\"/clean/\")) + len(os.listdir(val_dir+\"/with_carotid/\"))\n",
    "\n",
    "    epochs = 50\n",
    "    batch_size = 12\n",
    "\n",
    "     \n",
    "\n",
    "    train_images_labels = get_train_image_quant(train_data_dir) # for weights balancing\n",
    "    print (get_class_weights(train_images_labels))\n",
    "\n",
    "\n",
    "#layer_to_train = 3\n",
    "#for layer in model.layers[:layer_to_train]:\n",
    "#        if not isinstance(layer, BatchNormalization):\n",
    "#            layer.trainable = True  \n",
    "\n",
    "    dependencies = {\n",
    "            'recall_metric': recall_metric,\n",
    "            \"f1_metric\": f1_metric\n",
    "        }\n",
    "\n",
    "    model = load_model(previous_model_path, custom_objects=dependencies)\n",
    "\n",
    "\n",
    "    model.trainable = True\n",
    "    #model.layers[-3:].trainable = True\n",
    "\n",
    "    opt =  tensorflow.keras.optimizers.Adam(1e-6)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy',f1_metric])\n",
    "\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        # horizontal_flip=True,\n",
    "        # vertical_flip=True,\n",
    "         rescale=1. / 255,\n",
    "      #   preprocessing_function=add_noise,\n",
    "         width_shift_range=[0,40], \n",
    "         zoom_range=[0.9,1.1],\n",
    "          rotation_range=15,\n",
    "          shear_range=0.1,\n",
    "           fill_mode='constant', cval=0\n",
    "           \n",
    "    )\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0.0001, \\\n",
    "        patience=25, verbose=1, mode='auto')\n",
    "\n",
    "    checkpoint = ModelCheckpoint(current_model_path, monitor='val_accuracy',verbose=1,   # val_recall_metric\n",
    "                                    save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "    log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,  histogram_freq=1, write_graph=True, write_images=True) \n",
    "\n",
    "#class_weights = dict(zip(np.unique(train_generator.classes), class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=300,#epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[ early,checkpoint],\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples//batch_size,\n",
    "    class_weight=get_class_weights(train_images_labels)\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix,accuracy_score\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tensorflow.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tensorflow.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tensorflow.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tensorflow.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tensorflow.newaxis]\n",
    "    heatmap = tensorflow.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tensorflow.maximum(heatmap, 0) / tensorflow.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "#map grad overly\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "def prepare_img(img, img_width, img_height,channels):\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape(1, img_width, img_height, channels).astype('float32')\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def calc_results_onTest(Testpath,modelPath,fold_num):\n",
    "    channels =3\n",
    "    img_width, img_height = 400, 400\n",
    "\n",
    "\n",
    "\n",
    "    dependencies = {\n",
    "            'recall_metric': recall_metric,\n",
    "            \"f1_metric\": f1_metric\n",
    "        }\n",
    "\n",
    "    model = load_model(modelPath, custom_objects=dependencies)\n",
    "\n",
    "    model1 = model\n",
    "\n",
    "# Remove last layer's softmax\n",
    "    #model1.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "\n",
    "#print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
    "# Generate class activation heatmap\n",
    "    path = Testpath\n",
    "\n",
    "    y_preds,y_true = [],[]\n",
    "    for class_type in [\"with_carotid/\",\"clean/\"]:\n",
    "      file_list = os.listdir(path+class_type)\n",
    "\n",
    "\n",
    "  \n",
    "      counter=0\n",
    "      for filename in file_list:\n",
    "    #print(\"open\",filename )\n",
    "        counter+=1\n",
    "        img = load_img(path+class_type+filename, grayscale=False, target_size=(img_width, img_height))\n",
    "    #plt.matshow(img)\n",
    "    #plt.show()\n",
    "        img = prepare_img(img,img_width, img_height,channels)\n",
    "        preds = model1.predict(img)\n",
    "        print(counter,\" \", filename,np.argmax(preds))\n",
    "        y_preds.append(np.argmax(preds))\n",
    "        if \"with_carotid\" in class_type:y_true.append(1)\n",
    "        else: y_true.append(0)\n",
    "    #heatmap = make_gradcam_heatmap(img, model1.get_layer('densenet121'), 'conv5_block16_concat',1)\n",
    "\n",
    "# Display heatmap\n",
    "    #plt.matshow(heatmap)\n",
    "    #plt.show()\n",
    "\n",
    "    #save_and_display_gradcam(path+class_type+filename, heatmap)\n",
    "    if os.path.exists(\"output_CV_01.txt\"):\n",
    "        f=open(\"output_CV_01.txt\", 'a')\n",
    "    else:\n",
    "        f=open(\"output_CV_01.txt\", 'w+')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"---------------------\"+str(fold_num)+\"---------------------\"+\"\\n\")\n",
    "    f.write(\"y_preds: \"+ str(y_preds)+\"\\n\")\n",
    "    f.write(\"y_true\" + str(y_true)+\"\\n\")\n",
    "\n",
    "    f.write(\"recall: \" + str(recall_score(y_true, y_preds))+\"\\n\")\n",
    "    f.write(\"precision: \"+ str(precision_score(y_true, y_preds))+\"\\n\")\n",
    "    f.write(\"F1: \"+ str(f1_score(y_true, y_preds))+\"\\n\")\n",
    "    f.write(\"accuracy: \"+ str(accuracy_score(y_true, y_preds))+\"\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    f.write(\"specificity: \" +str(specificity)+\"\\n\")\n",
    "    f.write(\"confusion: \"+\"\\n\")\n",
    "    f.write(str(confusion_matrix(y_true, y_preds))+\"\\n\")\n",
    "    f.write(\"------------------------------------------\"+\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "panoram/CV_7/fold_1/train/\n",
      "panoram/CV_7/fold_1/val/\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Found 371 images belonging to 2 classes.\n",
      "Found 144 images belonging to 2 classes.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Functio (None, 11, 11, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 185856)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              190317568 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 244,917,218\n",
      "Trainable params: 190,580,482\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 72s 2s/step - loss: 2.0842 - accuracy: 0.5408 - f1_metric: 0.5335 - val_loss: 0.4390 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.85417, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 39s 2s/step - loss: 1.2574 - accuracy: 0.6254 - f1_metric: 0.6268 - val_loss: 0.4012 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.85417 to 0.85417, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 40s 2s/step - loss: 1.0724 - accuracy: 0.6197 - f1_metric: 0.6096 - val_loss: 0.4143 - val_accuracy: 0.8264 - val_f1_metric: 0.8264\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.85417\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.8869 - accuracy: 0.6817 - f1_metric: 0.6694 - val_loss: 0.3911 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.85417\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.9275 - accuracy: 0.6958 - f1_metric: 0.6947 - val_loss: 0.5273 - val_accuracy: 0.7500 - val_f1_metric: 0.7500\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.85417\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 37s 2s/step - loss: 0.6433 - accuracy: 0.7437 - f1_metric: 0.7409 - val_loss: 0.3891 - val_accuracy: 0.7917 - val_f1_metric: 0.7917\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.85417\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 36s 2s/step - loss: 0.6242 - accuracy: 0.7662 - f1_metric: 0.7745 - val_loss: 0.3686 - val_accuracy: 0.7986 - val_f1_metric: 0.7986\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.85417\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.6297 - accuracy: 0.7521 - f1_metric: 0.7491 - val_loss: 0.6972 - val_accuracy: 0.6528 - val_f1_metric: 0.6528\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.85417\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.5980 - accuracy: 0.7690 - f1_metric: 0.7654 - val_loss: 1.0215 - val_accuracy: 0.5347 - val_f1_metric: 0.5347\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.85417\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.4813 - accuracy: 0.7915 - f1_metric: 0.7989 - val_loss: 0.5273 - val_accuracy: 0.7222 - val_f1_metric: 0.7222\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.85417\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 38s 2s/step - loss: 0.5251 - accuracy: 0.8056 - f1_metric: 0.8007 - val_loss: 0.3698 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.85417\n",
      "Epoch 00011: early stopping\n",
      "--------------------------------END FIRST TRAIN fold:1 ----------------------------------------------\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Functio (None, 11, 11, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 185856)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              190317568 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 244,917,218\n",
      "Trainable params: 244,856,674\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 371 images belonging to 2 classes.\n",
      "Found 144 images belonging to 2 classes.\n",
      "Epoch 1/300\n",
      "30/30 [==============================] - 65s 1s/step - loss: 0.9433 - accuracy: 0.6852 - f1_metric: 0.6851 - val_loss: 0.5421 - val_accuracy: 0.7361 - val_f1_metric: 0.7361\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.73611, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 2/300\n",
      "30/30 [==============================] - 46s 2s/step - loss: 0.8833 - accuracy: 0.6184 - f1_metric: 0.6194 - val_loss: 0.5565 - val_accuracy: 0.7292 - val_f1_metric: 0.7292\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.73611\n",
      "Epoch 3/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.7728 - accuracy: 0.6825 - f1_metric: 0.6828 - val_loss: 0.5501 - val_accuracy: 0.7292 - val_f1_metric: 0.7292\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.73611\n",
      "Epoch 4/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.6397 - accuracy: 0.7270 - f1_metric: 0.7265 - val_loss: 0.5272 - val_accuracy: 0.7431 - val_f1_metric: 0.7431\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.73611 to 0.74306, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 5/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.6407 - accuracy: 0.7047 - f1_metric: 0.7040 - val_loss: 0.5042 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.74306 to 0.77083, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 6/300\n",
      "30/30 [==============================] - 47s 2s/step - loss: 0.6380 - accuracy: 0.7382 - f1_metric: 0.7376 - val_loss: 0.5084 - val_accuracy: 0.7639 - val_f1_metric: 0.7639\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.77083\n",
      "Epoch 7/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.5570 - accuracy: 0.7493 - f1_metric: 0.7490 - val_loss: 0.4910 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.77083\n",
      "Epoch 8/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.5808 - accuracy: 0.7688 - f1_metric: 0.7689 - val_loss: 0.4784 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.77083\n",
      "Epoch 9/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.5387 - accuracy: 0.7744 - f1_metric: 0.7740 - val_loss: 0.5089 - val_accuracy: 0.7569 - val_f1_metric: 0.7569\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.77083\n",
      "Epoch 10/300\n",
      "30/30 [==============================] - 45s 1s/step - loss: 0.4993 - accuracy: 0.7660 - f1_metric: 0.7657 - val_loss: 0.4724 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.77083\n",
      "Epoch 11/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.4590 - accuracy: 0.8078 - f1_metric: 0.8076 - val_loss: 0.4515 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.77083\n",
      "Epoch 12/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.4690 - accuracy: 0.8050 - f1_metric: 0.8056 - val_loss: 0.4927 - val_accuracy: 0.7569 - val_f1_metric: 0.7569\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.77083\n",
      "Epoch 13/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.4552 - accuracy: 0.8134 - f1_metric: 0.8134 - val_loss: 0.5065 - val_accuracy: 0.7361 - val_f1_metric: 0.7361\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.77083\n",
      "Epoch 14/300\n",
      "30/30 [==============================] - 44s 1s/step - loss: 0.4138 - accuracy: 0.8384 - f1_metric: 0.8389 - val_loss: 0.4470 - val_accuracy: 0.7847 - val_f1_metric: 0.7847\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.77083 to 0.78472, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 15/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.4095 - accuracy: 0.8357 - f1_metric: 0.8359 - val_loss: 0.4495 - val_accuracy: 0.7778 - val_f1_metric: 0.7778\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.78472\n",
      "Epoch 16/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.3884 - accuracy: 0.8468 - f1_metric: 0.8467 - val_loss: 0.3905 - val_accuracy: 0.8333 - val_f1_metric: 0.8333\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.78472 to 0.83333, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 17/300\n",
      "30/30 [==============================] - 47s 2s/step - loss: 0.3656 - accuracy: 0.8468 - f1_metric: 0.8467 - val_loss: 0.3940 - val_accuracy: 0.8333 - val_f1_metric: 0.8333\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.83333\n",
      "Epoch 18/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.3516 - accuracy: 0.8635 - f1_metric: 0.8636 - val_loss: 0.4190 - val_accuracy: 0.7986 - val_f1_metric: 0.7986\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.83333\n",
      "Epoch 19/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.3313 - accuracy: 0.8607 - f1_metric: 0.8604 - val_loss: 0.4120 - val_accuracy: 0.7847 - val_f1_metric: 0.7847\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.83333\n",
      "Epoch 20/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.3310 - accuracy: 0.8691 - f1_metric: 0.8689 - val_loss: 0.3969 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.83333\n",
      "Epoch 21/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.2927 - accuracy: 0.8774 - f1_metric: 0.8775 - val_loss: 0.3799 - val_accuracy: 0.8194 - val_f1_metric: 0.8194\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.83333\n",
      "Epoch 22/300\n",
      "30/30 [==============================] - 44s 1s/step - loss: 0.2831 - accuracy: 0.9053 - f1_metric: 0.9048 - val_loss: 0.3917 - val_accuracy: 0.7986 - val_f1_metric: 0.7986\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.83333\n",
      "Epoch 23/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.2692 - accuracy: 0.8997 - f1_metric: 0.9000 - val_loss: 0.3826 - val_accuracy: 0.8194 - val_f1_metric: 0.8194\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.83333\n",
      "Epoch 24/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.2556 - accuracy: 0.9081 - f1_metric: 0.9078 - val_loss: 0.3625 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.83333 to 0.84028, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 25/300\n",
      "30/30 [==============================] - 44s 1s/step - loss: 0.2456 - accuracy: 0.9304 - f1_metric: 0.9303 - val_loss: 0.4050 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.84028\n",
      "Epoch 26/300\n",
      "30/30 [==============================] - 39s 1s/step - loss: 0.2352 - accuracy: 0.9248 - f1_metric: 0.9250 - val_loss: 0.3348 - val_accuracy: 0.8681 - val_f1_metric: 0.8681\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.84028 to 0.86806, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 27/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.2039 - accuracy: 0.9443 - f1_metric: 0.9444 - val_loss: 0.4083 - val_accuracy: 0.7986 - val_f1_metric: 0.7986\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.86806\n",
      "Epoch 28/300\n",
      "30/30 [==============================] - 39s 1s/step - loss: 0.2242 - accuracy: 0.9164 - f1_metric: 0.9162 - val_loss: 0.3692 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.86806\n",
      "Epoch 29/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.2182 - accuracy: 0.9164 - f1_metric: 0.9159 - val_loss: 0.4164 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.86806\n",
      "Epoch 30/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.1668 - accuracy: 0.9331 - f1_metric: 0.9331 - val_loss: 0.3228 - val_accuracy: 0.8819 - val_f1_metric: 0.8819\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.86806 to 0.88194, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 31/300\n",
      "30/30 [==============================] - 45s 1s/step - loss: 0.1893 - accuracy: 0.9359 - f1_metric: 0.9361 - val_loss: 0.3818 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.88194\n",
      "Epoch 32/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.1664 - accuracy: 0.9331 - f1_metric: 0.9333 - val_loss: 0.3485 - val_accuracy: 0.8681 - val_f1_metric: 0.8681\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.88194\n",
      "Epoch 33/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1750 - accuracy: 0.9443 - f1_metric: 0.9444 - val_loss: 0.3893 - val_accuracy: 0.8264 - val_f1_metric: 0.8264\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.88194\n",
      "Epoch 34/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.1700 - accuracy: 0.9415 - f1_metric: 0.9414 - val_loss: 0.3792 - val_accuracy: 0.8333 - val_f1_metric: 0.8333\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.88194\n",
      "Epoch 35/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.1400 - accuracy: 0.9499 - f1_metric: 0.9500 - val_loss: 0.3695 - val_accuracy: 0.8472 - val_f1_metric: 0.8472\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.88194\n",
      "Epoch 36/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.1208 - accuracy: 0.9638 - f1_metric: 0.9639 - val_loss: 0.3765 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.88194\n",
      "Epoch 37/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1124 - accuracy: 0.9554 - f1_metric: 0.9556 - val_loss: 0.3982 - val_accuracy: 0.8333 - val_f1_metric: 0.8333\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.88194\n",
      "Epoch 38/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.1053 - accuracy: 0.9638 - f1_metric: 0.9636 - val_loss: 0.3731 - val_accuracy: 0.8819 - val_f1_metric: 0.8819\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.88194\n",
      "Epoch 39/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.1108 - accuracy: 0.9694 - f1_metric: 0.9694 - val_loss: 0.3883 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.88194\n",
      "Epoch 40/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1104 - accuracy: 0.9582 - f1_metric: 0.9583 - val_loss: 0.3470 - val_accuracy: 0.8958 - val_f1_metric: 0.8958\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.88194 to 0.89583, saving model to panoram/CV_7/CV7_1.h5\n",
      "Epoch 41/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.1320 - accuracy: 0.9443 - f1_metric: 0.9439 - val_loss: 0.3989 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.89583\n",
      "Epoch 42/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.1004 - accuracy: 0.9666 - f1_metric: 0.9667 - val_loss: 0.3929 - val_accuracy: 0.8681 - val_f1_metric: 0.8681\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.89583\n",
      "Epoch 43/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.0974 - accuracy: 0.9694 - f1_metric: 0.9694 - val_loss: 0.3666 - val_accuracy: 0.8889 - val_f1_metric: 0.8889\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.89583\n",
      "Epoch 44/300\n",
      "30/30 [==============================] - 45s 2s/step - loss: 0.0657 - accuracy: 0.9889 - f1_metric: 0.9889 - val_loss: 0.4233 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.89583\n",
      "Epoch 45/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.0758 - accuracy: 0.9721 - f1_metric: 0.9722 - val_loss: 0.4074 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.89583\n",
      "Epoch 46/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.0710 - accuracy: 0.9805 - f1_metric: 0.9806 - val_loss: 0.3913 - val_accuracy: 0.8819 - val_f1_metric: 0.8819\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.89583\n",
      "Epoch 47/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.0612 - accuracy: 0.9833 - f1_metric: 0.9833 - val_loss: 0.4563 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.89583\n",
      "Epoch 48/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.0498 - accuracy: 0.9805 - f1_metric: 0.9806 - val_loss: 0.4030 - val_accuracy: 0.8958 - val_f1_metric: 0.8958\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.89583\n",
      "Epoch 49/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.0624 - accuracy: 0.9805 - f1_metric: 0.9806 - val_loss: 0.4558 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.89583\n",
      "Epoch 50/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.0693 - accuracy: 0.9805 - f1_metric: 0.9806 - val_loss: 0.4027 - val_accuracy: 0.8958 - val_f1_metric: 0.8958\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.89583\n",
      "Epoch 51/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.0621 - accuracy: 0.9833 - f1_metric: 0.9833 - val_loss: 0.4265 - val_accuracy: 0.8819 - val_f1_metric: 0.8819\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.89583\n",
      "Epoch 52/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.0736 - accuracy: 0.9749 - f1_metric: 0.9750 - val_loss: 0.4748 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.89583\n",
      "Epoch 53/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.0380 - accuracy: 0.9944 - f1_metric: 0.9944 - val_loss: 0.4243 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.89583\n",
      "Epoch 54/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.0514 - accuracy: 0.9833 - f1_metric: 0.9833 - val_loss: 0.4393 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.89583\n",
      "Epoch 55/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.0306 - accuracy: 0.9944 - f1_metric: 0.9944 - val_loss: 0.5103 - val_accuracy: 0.8194 - val_f1_metric: 0.8194\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.89583\n",
      "Epoch 00055: early stopping\n",
      "--------------------------------END SECOND fold:1 ----------------------------------------------\n",
      "1   L_10481378.png 0\n",
      "2   L_10499888.png 0\n",
      "3   L_10536647.png 1\n",
      "4   L_10547353.png 1\n",
      "5   L_10566487.png 0\n",
      "6   L_10710947.png 1\n",
      "7   L_10714023.png 0\n",
      "8   L_10732989.png 1\n",
      "9   L_10751774.png 1\n",
      "10   L_10752818.png 1\n",
      "11   L_10758560.png 1\n",
      "12   L_2352437653.png 1\n",
      "13   L_2894532739.png 1\n",
      "14   L_2967446365.png 1\n",
      "15   L_3985356298.png 1\n",
      "16   R_0077502096.png 1\n",
      "17   R_10521526.png 1\n",
      "18   R_10535963.png 1\n",
      "19   R_10557294.png 0\n",
      "20   R_10678410.png 1\n",
      "21   R_10730654.png 1\n",
      "22   R_10752818.png 1\n",
      "23   R_2087211143.png 1\n",
      "24   R_2967446365.png 1\n",
      "1   L_0076411818.png 0\n",
      "2   L_0078506593.png 0\n",
      "3   L_0225708589.png 0\n",
      "4   L_0438081770.png 0\n",
      "5   L_0716639063.png 0\n",
      "6   L_0890095850.png 0\n",
      "7   L_10487749.png 0\n",
      "8   L_10488776.png 0\n",
      "9   L_10511339.png 0\n",
      "10   L_10535958.png 0\n",
      "11   L_10546420.png 0\n",
      "12   L_10547116.png 0\n",
      "13   L_10554317.png 0\n",
      "14   L_10554328.png 0\n",
      "15   L_10561693.png 0\n",
      "16   L_10574167.png 0\n",
      "17   L_10586900.png 0\n",
      "18   L_10589971.png 0\n",
      "19   L_10598246.png 0\n",
      "20   L_10598766.png 0\n",
      "21   L_10598786.png 0\n",
      "22   L_10604106.png 0\n",
      "23   L_10644244.png 0\n",
      "24   L_10671948.png 0\n",
      "25   L_10678920.png 0\n",
      "26   L_10685229.png 0\n",
      "27   L_10709942.png 0\n",
      "28   L_10711028.png 0\n",
      "29   L_10712735.png 0\n",
      "30   L_10713082.png 0\n",
      "31   L_10713758.png 0\n",
      "32   L_10714180.png 0\n",
      "33   L_10715785.png 0\n",
      "34   L_10718150.png 0\n",
      "35   L_10718593.png 0\n",
      "36   L_10719052.png 0\n",
      "37   L_10721346.png 0\n",
      "38   L_10729099.png 0\n",
      "39   L_10730621.png 1\n",
      "40   L_10731260.png 0\n",
      "41   L_10735389.png 0\n",
      "42   L_10736283.png 0\n",
      "43   L_10737411.png 0\n",
      "44   L_10738574.png 0\n",
      "45   L_10740869.png 0\n",
      "46   L_10742062.png 0\n",
      "47   L_10743746.png 0\n",
      "48   L_10745390.png 0\n",
      "49   L_10745624.png 0\n",
      "50   L_10746187.png 0\n",
      "51   L_10749905.png 0\n",
      "52   L_10754108.png 0\n",
      "53   L_10757388.png 0\n",
      "54   L_10757823.png 0\n",
      "55   L_10757976.png 0\n",
      "56   L_10758962.png 0\n",
      "57   L_10760871.png 0\n",
      "58   L_10761271.png 0\n",
      "59   L_10799251.png 0\n",
      "60   L_1811210579.png 0\n",
      "61   L_2676177294.png 0\n",
      "62   L_3036417066.png 0\n",
      "63   L_3768512571.png 0\n",
      "64   L_3785141456.png 0\n",
      "65   L_4198855688.png 0\n",
      "66   R_0225708589.png 0\n",
      "67   R_0429615736.png 0\n",
      "68   R_10477721.png 0\n",
      "69   R_10484749.png 0\n",
      "70   R_10508478.png 1\n",
      "71   R_10515760.png 0\n",
      "72   R_10551769.png 0\n",
      "73   R_10554317.png 0\n",
      "74   R_10571056.png 0\n",
      "75   R_10581482.png 0\n",
      "76   R_10586313.png 0\n",
      "77   R_10598246.png 0\n",
      "78   R_10598302.png 0\n",
      "79   R_10604618.png 1\n",
      "80   R_10624765.png 0\n",
      "81   R_10627695.png 0\n",
      "82   R_10644225.png 0\n",
      "83   R_10652333.png 0\n",
      "84   R_10655392.png 0\n",
      "85   R_10658134.png 0\n",
      "86   R_10709514.png 0\n",
      "87   R_10714477.png 0\n",
      "88   R_10720046.png 0\n",
      "89   R_10733723.png 0\n",
      "90   R_10733730.png 0\n",
      "91   R_10736283.png 0\n",
      "92   R_10739320.png 0\n",
      "93   R_10742062.png 0\n",
      "94   R_10742486.png 1\n",
      "95   R_10742591.png 0\n",
      "96   R_10743746.png 0\n",
      "97   R_10745390.png 0\n",
      "98   R_10749049.png 0\n",
      "99   R_10749905.png 0\n",
      "100   R_10751960.png 0\n",
      "101   R_10752071.png 0\n",
      "102   R_10755311.png 0\n",
      "103   R_10755946.png 0\n",
      "104   R_10756152.png 0\n",
      "105   R_10757231.png 0\n",
      "106   R_10757388.png 0\n",
      "107   R_10757720.png 0\n",
      "108   R_10757976.png 0\n",
      "109   R_10758706.png 0\n",
      "110   R_10758962.png 0\n",
      "111   R_10759152.png 0\n",
      "112   R_10759419.png 0\n",
      "113   R_10760871.png 0\n",
      "114   R_10762418.png 0\n",
      "115   R_10763389.png 0\n",
      "116   R_10764669.png 0\n",
      "117   R_2478345063.png 0\n",
      "118   R_2676177294.png 0\n",
      "119   R_3276214298.png 0\n",
      "120   R_3785141456.png 0\n",
      "2\n",
      "panoram/CV_7/fold_2/train/\n",
      "panoram/CV_7/fold_2/val/\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Found 371 images belonging to 2 classes.\n",
      "Found 144 images belonging to 2 classes.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Functio (None, 11, 11, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 185856)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              190317568 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 244,917,218\n",
      "Trainable params: 190,580,482\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "23/23 [==============================] - 57s 2s/step - loss: 2.1669 - accuracy: 0.5775 - f1_metric: 0.5688 - val_loss: 0.8066 - val_accuracy: 0.6667 - val_f1_metric: 0.6667\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.66667, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.1555 - accuracy: 0.6620 - f1_metric: 0.6621 - val_loss: 0.4892 - val_accuracy: 0.8194 - val_f1_metric: 0.8194\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.66667 to 0.81944, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.9194 - accuracy: 0.7070 - f1_metric: 0.6938 - val_loss: 1.1002 - val_accuracy: 0.5625 - val_f1_metric: 0.5625\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.81944\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 39s 2s/step - loss: 0.8237 - accuracy: 0.7127 - f1_metric: 0.7228 - val_loss: 1.2423 - val_accuracy: 0.5833 - val_f1_metric: 0.5833\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.81944\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.9036 - accuracy: 0.7268 - f1_metric: 0.7246 - val_loss: 0.9662 - val_accuracy: 0.6042 - val_f1_metric: 0.6042\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.81944\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.7257 - accuracy: 0.7268 - f1_metric: 0.7364 - val_loss: 0.4802 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.81944\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5763 - accuracy: 0.7746 - f1_metric: 0.7826 - val_loss: 0.7254 - val_accuracy: 0.6736 - val_f1_metric: 0.6736\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.81944\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 39s 2s/step - loss: 0.5863 - accuracy: 0.7437 - f1_metric: 0.7527 - val_loss: 0.7510 - val_accuracy: 0.7292 - val_f1_metric: 0.7292\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.81944\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 45s 2s/step - loss: 0.5041 - accuracy: 0.7962 - f1_metric: 0.7962 - val_loss: 0.7034 - val_accuracy: 0.7361 - val_f1_metric: 0.7361\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.81944\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 45s 2s/step - loss: 0.4271 - accuracy: 0.8479 - f1_metric: 0.8533 - val_loss: 0.6104 - val_accuracy: 0.7708 - val_f1_metric: 0.7708\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.81944\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 47s 2s/step - loss: 0.4996 - accuracy: 0.7859 - f1_metric: 0.7935 - val_loss: 0.3826 - val_accuracy: 0.8472 - val_f1_metric: 0.8472\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.81944 to 0.84722, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 45s 2s/step - loss: 0.4609 - accuracy: 0.8197 - f1_metric: 0.8143 - val_loss: 0.5209 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.84722\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 43s 2s/step - loss: 0.4313 - accuracy: 0.8310 - f1_metric: 0.8370 - val_loss: 0.3927 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.84722\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 44s 2s/step - loss: 0.3522 - accuracy: 0.8370 - f1_metric: 0.8370 - val_loss: 0.7024 - val_accuracy: 0.7569 - val_f1_metric: 0.7569\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.84722\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 43s 2s/step - loss: 0.3990 - accuracy: 0.8563 - f1_metric: 0.8614 - val_loss: 0.4275 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00015: val_f1_metric improved from 0.84722 to 0.85417, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 45s 2s/step - loss: 0.3728 - accuracy: 0.8676 - f1_metric: 0.8487 - val_loss: 0.3770 - val_accuracy: 0.8681 - val_f1_metric: 0.8681\n",
      "\n",
      "Epoch 00016: val_f1_metric improved from 0.85417 to 0.86806, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.3540 - accuracy: 0.8676 - f1_metric: 0.8487 - val_loss: 0.3813 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00017: val_f1_metric improved from 0.86806 to 0.87500, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 46s 2s/step - loss: 0.3625 - accuracy: 0.8535 - f1_metric: 0.8469 - val_loss: 0.6200 - val_accuracy: 0.7500 - val_f1_metric: 0.7500\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.87500\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4902 - accuracy: 0.8366 - f1_metric: 0.8424 - val_loss: 0.4172 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.87500\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 39s 2s/step - loss: 0.3595 - accuracy: 0.8507 - f1_metric: 0.8560 - val_loss: 0.6687 - val_accuracy: 0.7361 - val_f1_metric: 0.7361\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.87500\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.3170 - accuracy: 0.8696 - f1_metric: 0.8696 - val_loss: 0.4678 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.87500\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.2836 - accuracy: 0.8986 - f1_metric: 0.8904 - val_loss: 0.4853 - val_accuracy: 0.8472 - val_f1_metric: 0.8472\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.87500\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.2840 - accuracy: 0.8986 - f1_metric: 0.9022 - val_loss: 1.0206 - val_accuracy: 0.6458 - val_f1_metric: 0.6458\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.87500\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4169 - accuracy: 0.8507 - f1_metric: 0.8324 - val_loss: 0.4808 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.87500\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3313 - accuracy: 0.8620 - f1_metric: 0.8668 - val_loss: 0.6217 - val_accuracy: 0.7986 - val_f1_metric: 0.7986\n",
      "\n",
      "Epoch 00025: val_f1_metric did not improve from 0.87500\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.2522 - accuracy: 0.8930 - f1_metric: 0.8967 - val_loss: 0.4353 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00026: val_f1_metric did not improve from 0.87500\n",
      "Epoch 00026: early stopping\n",
      "--------------------------------END FIRST TRAIN fold:2 ----------------------------------------------\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Found 371 images belonging to 2 classes.\n",
      "{0: 240, 1: 131}\n",
      "{0: 0.7729166666666667, 1: 1.416030534351145}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Functio (None, 11, 11, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 185856)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              190317568 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 244,917,218\n",
      "Trainable params: 244,856,674\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 371 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 images belonging to 2 classes.\n",
      "Epoch 1/300\n",
      "30/30 [==============================] - 63s 2s/step - loss: 0.3917 - accuracy: 0.8747 - f1_metric: 0.8750 - val_loss: 0.5516 - val_accuracy: 0.7917 - val_f1_metric: 0.7917\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79167, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 2/300\n",
      "30/30 [==============================] - 43s 1s/step - loss: 0.2855 - accuracy: 0.8969 - f1_metric: 0.8970 - val_loss: 0.5608 - val_accuracy: 0.8056 - val_f1_metric: 0.8056\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.79167 to 0.80556, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 3/300\n",
      "30/30 [==============================] - 44s 1s/step - loss: 0.2645 - accuracy: 0.8914 - f1_metric: 0.8917 - val_loss: 0.4991 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.80556 to 0.84028, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 4/300\n",
      "30/30 [==============================] - 45s 1s/step - loss: 0.3031 - accuracy: 0.8830 - f1_metric: 0.8833 - val_loss: 0.4950 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.84028\n",
      "Epoch 5/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.2286 - accuracy: 0.9136 - f1_metric: 0.9139 - val_loss: 0.4467 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.84028\n",
      "Epoch 6/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.2075 - accuracy: 0.9304 - f1_metric: 0.9301 - val_loss: 0.4756 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.84028\n",
      "Epoch 7/300\n",
      "30/30 [==============================] - 38s 1s/step - loss: 0.1954 - accuracy: 0.9331 - f1_metric: 0.9328 - val_loss: 0.4827 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.84028\n",
      "Epoch 8/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1557 - accuracy: 0.9415 - f1_metric: 0.9414 - val_loss: 0.4069 - val_accuracy: 0.8611 - val_f1_metric: 0.8611\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.84028 to 0.86111, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 9/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.1828 - accuracy: 0.9499 - f1_metric: 0.9500 - val_loss: 0.4609 - val_accuracy: 0.8333 - val_f1_metric: 0.8333\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.86111\n",
      "Epoch 10/300\n",
      "30/30 [==============================] - 38s 1s/step - loss: 0.1850 - accuracy: 0.9276 - f1_metric: 0.9278 - val_loss: 0.4836 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.86111\n",
      "Epoch 11/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1586 - accuracy: 0.9359 - f1_metric: 0.9361 - val_loss: 0.4036 - val_accuracy: 0.8542 - val_f1_metric: 0.8542\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.86111\n",
      "Epoch 12/300\n",
      "30/30 [==============================] - 39s 1s/step - loss: 0.1626 - accuracy: 0.9387 - f1_metric: 0.9389 - val_loss: 0.4134 - val_accuracy: 0.8681 - val_f1_metric: 0.8681\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.86111 to 0.86806, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 13/300\n",
      "30/30 [==============================] - 42s 1s/step - loss: 0.1800 - accuracy: 0.9443 - f1_metric: 0.9437 - val_loss: 0.4712 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.86806\n",
      "Epoch 14/300\n",
      "30/30 [==============================] - 39s 1s/step - loss: 0.1393 - accuracy: 0.9526 - f1_metric: 0.9528 - val_loss: 0.4707 - val_accuracy: 0.8403 - val_f1_metric: 0.8403\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.86806\n",
      "Epoch 15/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1122 - accuracy: 0.9638 - f1_metric: 0.9636 - val_loss: 0.3997 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.86806 to 0.87500, saving model to panoram/CV_7/CV7_2.h5\n",
      "Epoch 16/300\n",
      "30/30 [==============================] - 41s 1s/step - loss: 0.1218 - accuracy: 0.9554 - f1_metric: 0.9556 - val_loss: 0.3797 - val_accuracy: 0.8750 - val_f1_metric: 0.8750\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87500\n",
      "Epoch 17/300\n",
      "30/30 [==============================] - 40s 1s/step - loss: 0.1031 - accuracy: 0.9694 - f1_metric: 0.9694 - val_loss: 0.4398 - val_accuracy: 0.8472 - val_f1_metric: 0.8472\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87500\n",
      "Epoch 18/300\n",
      "13/30 [============>.................] - ETA: 18s - loss: 0.1521 - accuracy: 0.9487 - f1_metric: 0.9487"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print(str(i))\n",
    "    train = \"panoram/CV_7/fold_\"+str(i)+\"/train/\"\n",
    "    val =   \"panoram/CV_7/fold_\"+str(i)+\"/val/\"\n",
    "    print(train)\n",
    "    print(val)\n",
    "    first_train(train,val, path_to_model=\"panoram/CV_7/CV7_\"+str(i) +\".h5\")\n",
    "    print(\"--------------------------------END FIRST TRAIN fold:\"+str(i)+\" ----------------------------------------------\")\n",
    "    fine_tuning_train(train,val, previous_model_path = \"panoram/CV_7/CV7_\"+str(i) +\".h5\", current_model_path = \"panoram/CV_7/CV7_\"+str(i) +\".h5\" )\n",
    "    print(\"--------------------------------END SECOND fold:\"+str(i)+\" ----------------------------------------------\")\n",
    "    \n",
    "    calc_results_onTest(Testpath=\"panoram/CV_7/fold_\"+str(i)+\"/test/\",modelPath=\"panoram/CV_7/CV7_\"+str(i) +\".h5\",fold_num=i)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow\n",
    "!pip install scipy\n",
    "\n",
    "#!pip install keras-tuner\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.path.exists(\"./CV_output.txt\"))\n",
    "f=open(\"output_CV.txt\", \"r\")\n",
    "for l in f:\n",
    "  print(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
